# 02 What to do if my network fails to train

è¿™ä¸€èŠ‚ä»‹ç»ä¸€ä¸ªåŸºæœ¬çš„è®­ç»ƒè¿‡ç¨‹é‡Œæœ‰å“ªäº›éœ€è¦æ³¨æ„å’Œå¯ä»¥æ”¹è¿›çš„åœ°æ–¹

## 1. General Guidance: åŸºæœ¬çš„æ¡†æ¶å’Œæ€æ ·æ›´å¥½çš„è®­ç»ƒ

### 1.1 Framework of ML

ä¸‰æ­¥è®­ç»ƒï¼š1 å†™å‡ºå«æœ‰æœªçŸ¥å‚æ•°çš„function 2 å†™å‡ºå…³äºæœªçŸ¥å‚æ•°çš„function -- loss function 3 Optimization

sample code --> simple baseline, but how should we improve it?

### 1.2 General Guide

å¦‚æœæ¨¡å‹é¢„æµ‹ç»“æœä¸å¥½ï¼Œå…ˆå»æ£€æŸ¥training lossï¼Œå†çœ‹testingç»“æœï¼Œå¦‚æœtraining losså¾ˆå¤§ï¼Œè®­ç»ƒé›†è®­ç»ƒçš„ä¸å¥½ï¼Œé‚£ä¹ˆï¼š

#### Trainingç»“æœä¸å¥½ï¼Œå¯èƒ½å‡ºç°çš„ä¸¤ä¸ªé—®é¢˜ï¼š

#### 1.2.1 Model Bias

æˆ–è€…ç”¨é€šä¿—çš„è¯æ¥è¯´ï¼Œæ¨¡å‹å¤ªè¿‡ç®€å•ï¼Œfunction seté‡Œç”šè‡³ä¸å«æœ‰èƒ½è®©lossè¶³å¤Ÿä½çš„å‡½æ•°

è§£å†³æ–¹æ³•ï¼š1 å¢åŠ features 2 **deep** learning

#### 1.2.2 Optimization Issue

é™·å…¥local minimaï¼Œè™½ç„¶æƒ³æ‰¾çš„functionåœ¨seté‡Œï¼Œå´åŠ¨å¼¹ä¸å¾—ï¼Œæ— æ³•æ‰¾å‡º

#### ä¸¤è€…æ€ä¹ˆåŒºåˆ†ï¼š

å…ˆè·‘ä¸€äº›shallower networks(é€‰æ‹©æ¯”è¾ƒå®¹æ˜“optimizeçš„ï¼Œé¿å…ä¼˜åŒ–å¤±è´¥)ï¼Œå¦‚æœåœ¨è¿™ä¸ªæ¯”è¾ƒå°æ¯”è¾ƒæµ…çš„networkå¢åŠ å±‚æ•°å’Œå¤æ‚åº¦ä¹‹åçš„modelï¼Œæ˜æ˜å¼¹æ€§æ›´å¤§æ›´å¤æ‚ï¼Œlosså´æ²¡æœ‰æ›´ä½ï¼Œè¿™å°±æ˜¯optimization issueã€‚

å‡è®¾training losså·²ç»å˜å°äº†ï¼Œä½†æ˜¯testing data losså¤§ï¼Œå°±çœŸçš„å¯èƒ½æ˜¯overfittingï¼š

#### Trainingçš„losså°ï¼Œtestingçš„losså¤§ï¼š

#### 1.2.3 Overfitting

è§£å†³åŠæ³•ï¼š

1 å¢åŠ è®­ç»ƒé›†ï¼šå¯ä»¥ä½¿ç”¨data augmentationæ¥æ‰©å……æ•°æ®

2 ä¸è¦è®©æ¨¡å‹å¤ªflexible: 1 less parameters (CNN) 2 less features 3 Early stopping 4 Regularization 5 Dropout (éƒ¨åˆ†ç¥ç»å…ƒç¦ç”¨)

#### Bias-Complexity Trade-off: 

#### 1.2.4 Cross Validation

training --> training + validation

å¦‚ä½•åˆç†åœ°åŒºåˆ†training setå’Œvalidation set --> N-fold validation: æŠŠè®­ç»ƒé›†åˆ‡æˆNç­‰ä»½ï¼Œæ‹¿å…¶ä¸­ä¸€ä¸ªå½“ä½œValidation setï¼Œé‡å¤Næ¬¡ï¼ŒæŠŠNä¸ªmodelåœ¨ç›¸åŒç¯å¢ƒä¸‹trainå’Œvalidéƒ½è·‘ä¸€æ¬¡ï¼Œæ¯ä¸ªmodelè¿™Nç§æ•°æ®é›†çš„ç»“æœéƒ½å¹³å‡èµ·æ¥ï¼Œçœ‹è°çš„æœ€å¥½

#### 1.2.5 Mismatch

è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å¸ƒä¸ä¸€æ ·ï¼Œè¿™æ ·çš„å®éªŒæ²¡æœ‰æ„ä¹‰



## 2 Optimization

### 2.1 Critical point

When gradient is smallï¼Œé™¤äº†æœ‰local minimaï¼Œè¿˜æœ‰saddle point

#### 2.1.1 å¦‚ä½•åˆ¤æ–­æ˜¯local minimaè¿˜æ˜¯saddle point

Î¸é™„è¿‘Lossæ³°å‹’å±•å¼€ --> Hessian Matrix H, åªéœ€è€ƒå¯ŸHçš„ç‰¹å¾å€¼ï¼š

1 eigen valueså…¨æ­£ï¼ŒH possitive definite æ­£å®šçŸ©é˜µ --> local minima

2 å…¨è´Ÿï¼Œlocal maxima

3 æœ‰æ­£æœ‰è´Ÿ --> saddle point

#### 2.1.2 Don't be afraid of saddle point

Î¸ <-- è´Ÿçš„eigen valueå¯¹åº”çš„eigen vector + Î¸' (è¿ç®—é‡å¤§ï¼Œå®é™…ä¸­æ²¡äººç”¨è¿™ä¸ª)

#### local minimaæ²¡æœ‰é‚£ä¹ˆå¸¸è§ï¼Œgardientå¾ˆå°ä¸å†updateå¾ˆå¤šæƒ…å†µä¸‹æ˜¯saddle point

### è§£å†³å¡åœ¨critical pointçš„åŠæ³•ï¼š

### 2.2 Batch

#### 2.2.1 Shuffle -- æ¯ä¸€ä¸ªEpochçš„Batchéƒ½ä¸ä¸€æ ·

#### 2.2.2 Small Batch vs Large Batch

1 å› ä¸ºGPUå¹¶è¡Œè®¡ç®—ï¼Œå¤§æ‰¹æ¬¡(ä¸æ˜¯å¤ªå¤§)å’Œå°æ‰¹æ¬¡å•æ¬¡æ—¶é—´å·®ä¸å¤šï¼Œä½†æ˜¯1ä¸ªEpochå¤§æ‰¹æ¬¡è€—æ—¶æ›´çŸ­

2 å°æ‰¹æ¬¡performanceæ›´å¥½

3 Noisy updateæœ‰åŠ©äºè®­ç»ƒï¼Œå› ä¸ºæ¯æ¬¡æ›´æ–°losså‡½æ•°éƒ½æœ‰å·®å¼‚ï¼Œä¸åŒbatchbutongloss functionï¼Œæ›´ä¸å®¹æ˜“å¡ä½

4 Noisy updateæœ‰åˆ©äºtestingï¼Œå› ä¸ºæ›´ä¸å®¹æ˜“overfittingï¼›è¿˜æœ‰æ›´å®¹æ˜“èµ°å…¥â€œå¹³åŸâ€çš„minimaã€

### 2.3 Monentum

åŠ ä¸Šä¸€ä¸ªæƒ¯æ€§ï¼Œè€ƒè™‘æ‰€æœ‰çš„gradients

### 2.4 Adaptive Learning Rate

#### 2.4.1 Training stuckä¸ä»£è¡¨small gradient

è€Œæœ‰å¯èƒ½æ˜¯æŒ¯è¡ --> learning rateè®¾å®šå¤ªå¤§

åŒæ—¶é‡‡ç”¨å›ºå®šçš„lrå¾ˆéš¾åˆ°è¾¾æœ€ä¼˜è§£ï¼š1 lrè¾ƒå¤§ --> æœ€ä¼˜è§£é™„è¿‘æ¥å›æ¨ªè·³ 2 lrè¾ƒå°ï¼Œæœ€å¼€å§‹æœç€æœ€ä¼˜è§£ç¨³å®šç§»åŠ¨ï¼Œé è¿‘åç§»åŠ¨ç¼“æ…¢

æ‰€ä»¥ï¼Œæ²¡æœ‰å“ªä¸ªlearning rateèƒ½ä¸€åŠ³æ°¸é€¸é€‚åº”æ‰€æœ‰çš„å‚æ•°æ›´æ–°é€Ÿåº¦

#### 2.4.2 Different parameters need different learning rates

$$
\theta_i^{t+1} \leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t} g_i^t
$$

**åŸºæœ¬åŸåˆ™ï¼š**

+ æŸä¸€ä¸ªæ–¹å‘ä¸Šgradientçš„å€¼å¾ˆå°,éå¸¸çš„å¹³å¦ â‡’ learning rateè°ƒå¤§ä¸€ç‚¹

+ æŸä¸€ä¸ªæ–¹å‘ä¸Šéå¸¸çš„é™¡å³­,å¡åº¦å¾ˆå¤§ â‡’ learning rateå¯ä»¥è®¾å°ä¸€ç‚¹

##### 1 Root mean square --> ç”¨äºAdagrad

$$
\sigma_i^t = \sqrt{\frac{1}{t+1} \sum_{i=0}^{t} \left(g_i^t\right)^2}
$$

ç¼ºç‚¹ï¼šç´¯ç§¯ --> ä¸èƒ½ â€œå®æ—¶â€ è€ƒè™‘æ¢¯åº¦çš„å˜åŒ–æƒ…å†µ

##### 2 RMSprop

$$
\sigma_i^t = \sqrt{\alpha (\sigma_i^{t-1})^2 + (1 - \alpha) (g_i^t)^2}
$$

æ·»åŠ å‚æ•° Î±ï¼ˆè¡¨ç¤ºå½“å‰æ¢¯åº¦å¤§å°å¯¹äº learning rate çš„å½±å“æ¯”é‡ï¼Œæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼ˆhyperparameter)

- Î± è®¾å¾ˆå°è¶‹è¿‘æ–¼0ï¼Œå°±ä»£è¡¨è¿™ä¸€æ­¥ç®—å‡ºçš„ gáµ¢ ç›¸è¾ƒäºä¹‹å‰æ‰€ç®—å‡ºæ¥çš„ gradient è€Œè¨€æ¯”è¾ƒé‡è¦

- Î± è®¾å¾ˆå¤§è¶‹è¿‘æ–¼1ï¼Œå°±ä»£è¡¨ç°åœ¨ç®—å‡ºæ¥çš„ gáµ¢ æ¯”è¾ƒä¸é‡è¦ï¼Œä¹‹å‰ç®—å‡ºæ¥çš„ gradient æ¯”è¾ƒé‡è¦

##### 3 Adam = RMSprop + Momentum æœ€å¸¸ç”¨çš„optimizer

å…¶ä¸­ä½¿ç”¨ Pytorch é¢„è®¾çš„å‚æ•°å°±ä¼šæœ‰å¾ˆå¥½çš„æ•ˆæœ

#### 2.4.3 Learning rate scheduling

è®­ç»ƒåˆ°åé¢æ—¶ï¼Œç”±äºæ¢¯åº¦çš„ç§¯ç´¯ï¼Œä¼šå¯¼è‡´æ•´ä½“çš„ learning rate æé€Ÿå¢åŠ ï¼Œä»è€Œå‘ç”ŸæŠ–åŠ¨ï¼Œä½†éšç€æ–°çš„å¤§æ¢¯åº¦çš„åŠ å…¥ï¼Œä¼šä½¿å¾—é€æ¸learning rateé™ä½ï¼Œæœ€ç»ˆæ¢¯åº¦é€æ¸å¹³ç¨³ã€‚

**è§£å†³æ–¹æ³•**ï¼šLearning Rate Scheduling â‡’ è®© LearningRate ä¸ â€œè®­ç»ƒæ—¶é—´â€ æœ‰å…³ã€‚å°†åˆ†å­ ä¹Ÿè¿›è¡Œè°ƒæ•´ï¼Œå°†å…¶å‡çº§ä¸ºä¸æ—¶é—´ç›¸å…³çš„ä¸€ä¸ªå˜é‡ ï¼ˆä½¿ç”¨ Warm Upçš„æ–¹å¼ï¼Œéšç€æ—¶é—´å…ˆå˜å¤§åå˜å°ï¼‰ã€‚

$$
\theta_i^{t+1} \leftarrow \theta_i^t - \frac{\eta^t}{\sigma_i^t} g_i^t
$$

##### Learning rate decay

- è¿™ç§ç­–ç•¥ç”¨äºåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­é€æ¸å‡å°‘å­¦ä¹ ç‡ã€‚éšç€è®­ç»ƒæ¥è¿‘ç›®æ ‡ï¼Œæˆ‘ä»¬å‡å°‘å­¦ä¹ ç‡ä»¥é˜²æ­¢è·³è¿‡æœ€ä¼˜è§£ã€‚è¿™æœ‰åŠ©äºåœ¨ä¼˜åŒ–çš„åæœŸæ›´ç²¾ç»†åœ°è°ƒæ•´å‚æ•°ï¼Œè¾¾åˆ°æ›´å¥½çš„ä¼˜åŒ–æ•ˆæœã€‚

##### Warm up

+ è¿™ç§ç­–ç•¥åœ¨è®­ç»ƒåˆæœŸå…ˆå¢å¤§å­¦ä¹ ç‡ï¼Œç„¶åå†å‡å°ã€‚åœ¨è®­ç»ƒåˆæœŸï¼Œç”±äºä¼°è®¡çš„ Ïƒå…·æœ‰è¾ƒå¤§çš„æ–¹å·®ï¼Œæ‰€ä»¥é‡‡ç”¨è¾ƒé«˜çš„å­¦ä¹ ç‡å¯ä»¥å¿«é€Ÿæ”¶æ•›åˆ°è¾ƒå¥½çš„åŒºåŸŸã€‚ä¹‹åå†é€æ¸å‡å°å­¦ä¹ ç‡ï¼Œä»¥ç»†åŒ–å¯¹å‚æ•°çš„è°ƒæ•´ã€‚
+ åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæˆ–å…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œè¾ƒå¤§çš„æ–¹å·®å¯èƒ½æ„å‘³ç€åˆå§‹é˜¶æ®µçš„æ¢¯åº¦ä¼°è®¡ä¸å¤Ÿå‡†ç¡®ï¼Œå‚æ•°æ›´æ–°æ–¹å‘å¯èƒ½ä¼šæœ‰è¾ƒå¤§çš„æ³¢åŠ¨ã€‚ä¸ºäº†åœ¨è¿™ç§æƒ…å†µä¸‹æ›´å¿«åœ°æ‰¾åˆ°ä¸€ä¸ªè¾ƒå¥½çš„å‚æ•°åŒºåŸŸï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡è¿›è¡Œå¿«é€Ÿæ¢ç´¢ã€‚è¿™å°±æ˜¯é¢„çƒ­ç­–ç•¥çš„æ ¸å¿ƒæ€æƒ³ã€‚

åœ¨è®­ç»ƒåˆæœŸï¼Œç”±äºå¯¹è¯¯å·®è¡¨é¢çŠ¶æ€çš„ä¼°è®¡è¿˜ä¸å¤Ÿç²¾ç¡®ï¼Œä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡å¯ä»¥é¿å…å‚æ•°èµ°å¾—å¤ªè¿œï¼Œä»è€Œé˜²æ­¢æ¨¡å‹è®­ç»ƒæ—¶å‡ºç°ä¸ç¨³å®šçš„æƒ…å†µã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œä¼°è®¡é€æ¸å˜å¾—å‡†ç¡®ï¼Œå¯ä»¥é€‚å½“æé«˜å­¦ä¹ ç‡ï¼Œä»¥æ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å‚æ•°ã€‚

### 2.5 Optimization Summary

$$
\theta_i^{t+1} \leftarrow \theta_i^t - \frac{\eta^t}{\sigma_i^t} m_i^t
$$



## 3 Batch Normalization

å½’ä¸€åŒ–çš„ç›®çš„ä¸»è¦æ˜¯ä¸ºäº†è®©æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¯¹äºä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–çš„æ¨¡å‹ï¼Œæ¯æ¬¡è¿­ä»£ä¼šæ‰¾åˆ°æ¢¯åº¦æœ€å¤§çš„æ–¹å‘è¿­ä»£æ›´æ–°æ¨¡å‹å‚æ•°ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ¨¡å‹çš„ç‰¹å¾å±æ€§é‡çº²ä¸ä¸€ï¼Œé‚£ä¹ˆå¯»æ±‚æœ€ä¼˜è§£çš„ç‰¹å¾ç©ºé—´ï¼Œå°±å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªæ¤­åœ†å½¢çš„ï¼Œå…¶ä¸­å¤§é‡å†ˆçš„å±æ€§å¯¹åº”çš„å‚æ•°æœ‰è¾ƒé•¿çš„è½´ã€‚åœ¨æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼šå‡ºç°æ›´æ–°è¿‡ç¨‹ä¸æ˜¯ä¸€ç›´æœå‘æå°ç‚¹æ›´æ–°çš„ï¼Œè€Œæ˜¯å‘ˆç°Zå­—å‹ã€‚ä½¿ç”¨äº†å½’ä¸€åŒ–å¯¹é½é‡çº²ä¹‹åï¼Œæ›´æ–°è¿‡ç¨‹å°±å˜æˆäº†åœ¨è¿‘ä¼¼åœ†å½¢ç©ºé—´ï¼Œä¸æ–­å‘åœ†å¿ƒï¼ˆæå€¼ç‚¹ï¼‰è¿­ä»£çš„è¿‡ç¨‹ã€‚

### 3.1 Feature normalization/scaling

meanæ˜¯ä¸€è¡Œæ•°æ®ç®—å‡ºæ¥çš„

$$
x_i^r \leftarrow \frac{x_i^r - m_i}{\sigma_i}
$$

In general, feature normalization makes gradient descent converge faster.

### 3.2 Considering Deep learning

normalization å¯ä»¥applyåœ¨activation function çš„input/outputï¼Œä½†ç°åœ¨æ¯”è¾ƒå¤šçš„æ˜¯**å¯¹activation functionçš„inputåšnormalization**

z -- sigmoid --> a

ä¸€ä¸ªbatchç®—meanå’Œdivation

$$
\tilde{z}^i = \frac{z^i - \mu}{\sigma}
$$

æœ‰æ—¶å€™ï¼Œä½ å¹¶ä¸å¸Œæœ›ä½ å¾—activation function inputçš„ mean=0ï¼Œ standard divation = 1ï¼Œæ‰€ä»¥ä½ å¯ä»¥åšä»¥ä¸‹æ“ä½œï¼ŒåŒæ—¶ä¹Ÿä¼šè·Ÿéšç½‘ç»œæ›´æ–°:

$$
\hat{z}^i = \gamma \odot \tilde{z}^i + \beta
$$

meanå’Œstandard divationå—dataå½±å“ï¼ŒÎ²å’ŒÎ³æ˜¯networkå­¦å‡ºæ¥çš„

### 3.3 Testing

We do not always have batch at testing stage.

Computing the moving average of ğ and ğˆ of the batches during training.

$$
\bar{\mu} \leftarrow p \bar{\mu} + (1 - p) \mu^t
$$

$$
\tilde{z} = \frac{z - \bar{\mu}}{\bar{\sigma}}
$$

### 3.4 Internal Covariate Shift

å¯¹æ¯ä¸€ä¸ªlayeråšfeature scalingå¯¹Deep learningä¸Šæ˜¯ç”±å¾ˆå¤§ä½œç”¨çš„ï¼Œä»–ä¼šè®©internal covariate shiftï¼ˆå†…éƒ¨åæ–¹å·®å¹³ç§»ï¼‰é—®é¢˜è½»å¾®ä¸€äº›ï¼Œå› ä¸ºä½¿å¾—æ¯å±‚è¾“å…¥åˆ†å¸ƒç¨³å®šã€‚ï¼ˆæœªå®Œå¾…ç»­ï¼‰